# hanafuda_rl

这是一个基于深度强化学习的 AI 项目, 旨在训练一个能够玩日本传统纸牌游戏 "花札"(Hanafuda Koi-Koi) 的智能体. 项目采用了现代强化学习框架和自我对弈设计. 

---

## **项目结构与代码文件解析**

```
hanafuda_rl/
├─ envs/
│  ├─ hanafuda_env.py     # Gymnasium 环境实现
│  └─ rules.py            # 核心游戏规则与逻辑引擎
├─ agents/
│  ├─ random_agent.py     # 随机智能体 (基线)
│  ├─ rule_agent.py       # 规则智能体 (基线)
│  └─ sb3_agent.py        # 对 SB3 模型的包装，用于评估和对战
├─ train/
│  ├─ train_sb3.py        # 核心训练脚本 (包含自我对弈循环)
│  └─ eval.py             # 模型评估脚本
└─ results/
   └─ models/, logs/
```

### **关键代码文件职责**

*   `envs/rules.py`
    *   实现花札 Koi-Koi 的全部底层逻辑, 负责管理牌组, 发牌, 判定动作合法性, 计算役种和分数, 并维护一个由 `turn_phase` 驱动的内部状态机. 该文件与任何强化学习库无关, 仅用于实现游戏规则. 

*   `envs/hanafuda_env.py`
    *   将 `rules.py` 中的游戏状态转换为 RL 智能体能够理解的, 标准化的 `Gymnasium` 观测和动作空间. 定义了 `Dict` 观测空间和 `Discrete` 动作空间, 并实现了 `reset`, `step` 等核心 API. 提供了 `get_action_mask()` 方法, 为 `MaskablePPO` 提供必要的动作掩码。

*   `train/train_sb3.py`
    *   编排整个强化学习训练流程, 使用 `Stable Baselines3 Contrib` 库中的 `MaskablePPO` 算法构建了一个自我对弈训练循环. 通过 `SelfPlayEnvWrapper` 将双人对战环境适配为标准 RL 算法可以处理的单智能体环境, 并通过多进程并行化 (`SubprocVecEnv`) 加速训练. 

*   `train/eval.py`
    *   通过与其他 AI 对战的方式评估模型性能, 可以给出胜率, 平局率, 平均打点等关键数据. 

---

## **强化学习架构**

### **观测空间 (`observation_space`) 设计**

为了向智能体提供全面且易于处理的游戏状态信息, 我们设计了一个结构化的 `gym.spaces.Dict` 观测空间, 空间的主要构成部分如下: 

*   **牌面信息 (`MultiBinary`)**:
    *   `hand`, `table`, `my_collected`, `opp_collected`: `MultiBinary(48)`, 每一维对应一张唯一的牌, 值为 `1` 表示该牌存在于相应区域, `0` 表示不存在. 
    *   `koikoi_flags`: `MultiBinary(2)`, 表示双方是否已叫牌. 

*   **离散状态信息 (`Discrete`)**:
    *   `drawn_card`: `Discrete(49)`, 表示从牌堆抽出的牌的 ID. `48`作为特殊值, 表示当前不处于抽牌阶段. 
    *   `turn_phase`: `Discrete(4)`, 直接暴露游戏引擎的内部状态机阶段 (出牌、抽牌、叫牌决策等). 

*   **归一化数值特征 (`Box`)**:
    *   `deck_remaining`: 牌堆剩余牌数的归一化值. 
    *   `current_scores`: 双方当前役种得分的 `tanh` 归一化值. 

*   **专家特征 (`Box`)**:
    *   `my_yaku_progress`, `opp_yaku_progress`: 11维的向量, 量化各个役种的完成进度. 

### **动作空间 (`action_space`) 与动作掩码设计**

*   **动作空间**: `gym.spaces.Discrete(38)`
    *   `动作 0-31`: 出牌.
    *   `动作 32-35`: 抽牌配对.
    *   `动作 36`: 不叫牌.
    *   `动作 37`: 叫牌.

*   **动作掩码**:
    *   一个 `bool` 向量, 用于判断动作的合法性. 

### **奖励函数设计**

为了有效引导智能体学习, 我们采用了结合中间奖励和最终奖励的策略: 

*   **最终奖励**: 在游戏结束时给予, 这是最主要的学习信号.
    *   **胜利**: `+ 最终得分`
    *   **失败**: `- 对手的最终得分`

*   **中间塑形奖励**: 在游戏过程中给予. 
    *   **凑成新役**: `+ 新役分数 * 0.1`
    *   **目的**: 提供更频繁的反馈信号, 鼓励智能体积极地凑成役种. 

---

## **训练策略与超参数**

### **算法与策略网络**

*   **算法**: `sb3_contrib.MaskablePPO`
    *   **选择理由**: `MaskablePPO` 原生支持动作掩码. 

*   **策略网络**: `MaskableMultiInputActorCriticPolicy`
    *   **选择理由**: 这是一个为 `MaskablePPO` 设计的 Actor-Critic 策略网络, 可以能自动处理 `Dict` 观测空间, 为字典中的每个键创建一个独立的特征提取器, 并将提取出的特征拼接后送入后续的策略和价值网络中. 

### **训练超参数**

以下是在`train_sb3.py`中使用的关键超参数: 

| 超参数 | 值  | 解释  |
| ------- | ------ | ------- |
| `N_ENVS` | 10  | 并行线程数  |
| `learning_rate` | `3e-4`  | 学习率 |
| `n_steps` | 2048 | 更新前的采样步数 |
| `batch_size` | 128  | 梯度下降时每个mini-batch的大小 |
| `n_epochs` | 10 | 使用采样数据进行优化的轮数 |
| `gamma`  | 0.99  | 折扣因子 |
| `clip_range`  | 0.2  | PPO裁剪目标函数的范围 |
| `TOTAL_TIMESTEPS` | `5,000,000` | 总训练步数 |

---

## **训练过程**

### **训练脚本**
*   **无自我对弈**: 将模型与随机智能体进行对战训练. 
*   **自我对弈**: 初始将模型与随机智能体进行对战训练, 若干步后保存模型并将对手更换为保存后的模型, 随后重复保存-对战循环. 

### **训练记录**
*   **平均对局时长**
![ep_len_mean](/result/figures/ep_len_mean.png)
*   **平均奖励**
![ep_rew_mean](/result/figures/ep_rew_mean.png)

---

## **实验结果**

为了全面评估模型性能, 我们设置了五个不同级别的智能体进行循环赛, 每组对战进行 10,000 局.

### **参评智能体定义**
*   **random**: 随机智能体. 在所有合法动作中随机地选择一个, 作为性能的最低基准.
*   **rule**: 规则智能体. 当可以出牌时, 选择第一张可以与场上牌进行配对的牌; 当可以配对时, 选择场上第一张可以配对的牌进行配对; 当可以叫牌时, 选择直接结束游戏. 
*   **100K**: RL 智能体. 训练了 10 万步, 未经过自我对弈. 
*   **5M**: RL 智能体. 训练了 500 万步, 未经过自我对弈. 
*   **selfplay_5M**: RL 智能体. 使用自我对弈框架训练了 500 万步, 每 100 万步时保存一次模型并作为对手加入训练. 

### **胜率**
*下表中的行代表玩家，列代表对手。*
| 玩家 (Player) | vs random | vs rule | vs 100K | vs 5M | vs selfplay_5M |
| :--- | :---: | :---: | :---: | :---: | :---: |
| **random** | - | 14.47% | 19.88% | 18.85% | 16.83% |
| **rule** | 74.25% | - | 54.85% | 52.22% | 49.36% |
| **100K** | 61.46% | 35.84% | - | 42.64% | 40.13% |
| **5M** | 60.08% | 36.46% | 42.61% | - | 40.12% |
| **selfplay_5M** | 63.35% | 39.78% | 46.21% | 45.16% | - |

### **平均打点**
*正值表示行玩家平均每局赢的分数，负值表示输的分数。*
| 玩家 (Player) | vs random | vs rule | vs 100K | vs 5M | vs selfplay_5M |
| :--- | :---: | :---: | :---: | :---: | :---: |
| **random** | - | -1.91 | -1.96 | -2.55 | -2.49 |
| **rule** | 1.91 | - | 0.37 | 0.02 | -0.12 |
| **100K** | 1.96 | -0.37 | - | -0.33 | -0.51 |
| **5M** | 2.55 | -0.02 | 0.33 | - | -0.05 |
| **selfplay_5M** | 2.49 | 0.12 | 0.51 | 0.05 | - |